{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook contains the code written by Library of Congress, modified by me to fit this specific project, to bulk access LOC's digitized newspapers.\n",
    "Because my local computer does not have enough memory, the actual processing of this file was done using Duke Compute Clusters. See \"DCCProcessingFiles\" folder for code used in using DCC.\n",
    "\n",
    "Code provided by Library of Congress Chronicling American API guide here:\n",
    "https://libraryofcongress.github.io/data-exploration/loc.gov%20JSON%20API/Chronicling_America/ChronAm_analyzing_specific_titles_limit_results.html\n",
    "\n",
    "After modifying the code, I used the Duke Compute Cluster (see relevant folder) to obtain enough computing power to complete the data processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, re\n",
    "from urllib.request import urlopen\n",
    "import requests, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing search URL:\n",
    "searchURLTest = \"https://www.loc.gov/newspapers/?end_date=1930-01-01&ops=~10&qs=los+angeles+oil&searchType=advanced&start_date=1892-04-20&location_country=united+states&fo=json\"\n",
    "numberOfResults = 0\n",
    "\n",
    "def get_item_ids_test(url, items=[], conditional='True'):\n",
    "    global numberOfResults\n",
    "    # Check that the query URL is not an item or resource link.\n",
    "    exclude = [\"loc.gov/item\",\"loc.gov/resource\"]\n",
    "    if any(string in url for string in exclude):\n",
    "        raise NameError('Your URL points directly to an item or '\n",
    "                        'resource page (you can tell because \"item\" '\n",
    "                        'or \"resource\" is in the URL). Please use '\n",
    "                        'a search URL instead. For example, instead '\n",
    "                        'of \\\"https://www.loc.gov/item/2009581123/\\\", '\n",
    "                        'try \\\"https://www.loc.gov/maps/?q=2009581123\\\". ')\n",
    "\n",
    "    # request pages of 100 results at a time\n",
    "    params = {\"fo\": \"json\", \"c\": 100, \"at\": \"results,pagination\"}\n",
    "    call = requests.get(url, params=params)\n",
    "    # Check that the API request was successful\n",
    "    if (call.status_code==200) & ('json' in call.headers.get('content-type')):\n",
    "        data = call.json()\n",
    "        results = data['results'] # deleted the top 20 limit\n",
    "        for result in results:\n",
    "            # Filter out anything that's a collection or web page\n",
    "            filter_out = (\"collection\" in result.get(\"original_format\")) \\\n",
    "                    or (\"web page\" in result.get(\"original_format\")) \\\n",
    "                    or (eval(conditional)==False)\n",
    "            if not filter_out:\n",
    "                # Get the link to the item record\n",
    "                if result.get(\"id\"):\n",
    "                    item = result.get(\"id\")\n",
    "                    # Filter out links to Catalog or other platforms\n",
    "                    if item.startswith(\"http://www.loc.gov/resource\"):\n",
    "                      resource = item  # Assign item to resource\n",
    "                      items.append(resource)\n",
    "                    if item.startswith(\"http://www.loc.gov/item\"):\n",
    "                      items.append(item)\n",
    "            \n",
    "            numberOfResults += 1\n",
    "            print(f\"Processed {numberOfResults} results.\")\n",
    "            \n",
    "        # Repeat the loop on the next page, unless we're on the last page.\n",
    "        if data[\"pagination\"][\"next\"] is not None:\n",
    "            next_url = data[\"pagination\"][\"next\"]\n",
    "            print(f\"Total number of pages: {data['pagination']['total']}\")\n",
    "            get_item_ids_test(next_url, items, conditional)\n",
    "\n",
    "        return items\n",
    "    else:\n",
    "            print('There was a problem. Try running the cell again, or check your searchURL.')\n",
    "\n",
    "# Generate a list of records found from performing a query and save these Item IDs. (Create ids_list based on items found in the searchURL result)\n",
    "ids_list_test = get_item_ids_test(searchURLTest, items=[])\n",
    "\n",
    "# Add 'fo=json' to the end of each row in ids_list (All individual ids from from the ids_list are now listed in JSON format in new_ids)\n",
    "ids_list_json_test = []\n",
    "for id in ids_list_test:\n",
    "  if not id.endswith('&fo=json'):\n",
    "    id += '&fo=json'\n",
    "  ids_list_json_test.append(id)\n",
    "ids = ids_list_json_test\n",
    "\n",
    "print('\\nSuccess! Your API Search Query found '+str(len(ids_list_json_test))+' related newspaper pages. Proceed to the next step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries to store the item metadata\n",
    "item_metadata_list = []\n",
    "\n",
    "counter = 0\n",
    "# Iterate over the list of item IDs\n",
    "for item_id in ids_list_json_test:\n",
    "  item_response = requests.get(item_id)\n",
    "\n",
    "  # Check if the API call was successful and Parse the JSON response\n",
    "  if item_response.status_code == 200:\n",
    "    # Iterate over the ids_list_json list and extract the relevant metadata from each dictionary.\n",
    "    item_data = item_response.json()\n",
    "    # NOT filtering out newspapers that do not have a city associated with it.\n",
    "    # if 'location_city' not in item_data['item']:\n",
    "    #   continue\n",
    "\n",
    "    # Extract the relevant item metadata\n",
    "    Newspaper_Title = item_data['item']['newspaper_title']\n",
    "    Issue_Date = item_data['item']['date']\n",
    "    Page = item_data['pagination']['current']\n",
    "    State = item_data['item']['location_state']\n",
    "    City = item_data['item']['location_city']\n",
    "    LCCN = item_data['item']['number_lccn']\n",
    "    Contributor = item_data['item']['contributor_names']\n",
    "    Batch = item_data['item']['batch']\n",
    "    pdf = item_data['resource']['pdf']\n",
    "\n",
    "    # Add the item metadata to the list\n",
    "    item_metadata_list.append({\n",
    "        'Newspaper Title': Newspaper_Title,\n",
    "        'Issue Date': Issue_Date,\n",
    "        'Page Number': Page,\n",
    "        'LCCN': LCCN,\n",
    "        'City': City,\n",
    "        'State': State,\n",
    "        'Contributor': Contributor,\n",
    "        'Batch': Batch,\n",
    "        'PDF Link': pdf,\n",
    "    })\n",
    "    counter += 1\n",
    "    print(f\"Processed {counter} results.\")\n",
    "    \n",
    "\n",
    "# Change date format to MM-DD-YYYY\n",
    "for item in item_metadata_list:\n",
    "  item['Issue Date'] = pd.to_datetime(item['Issue Date']).strftime('%m-%d-%Y')\n",
    "\n",
    "# Create a Pandas DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(item_metadata_list)\n",
    "\n",
    "print('\\nSuccess! Ready to proceed to the next step!')\n",
    "\n",
    "# Add your Local saveTo Location (e.g. C:/Downloads/)\n",
    "saveTo = '/hpc/home/zz341/test4'\n",
    "\n",
    "# Set File Name. Make sure to rename the file so it doesn't overwrite previous!\n",
    "filename = 'LOCLAOilInitialExtract'\n",
    "\n",
    "metadata_dataframe = pd.DataFrame(item_metadata_list)\n",
    "metadata_dataframe.to_csv(saveTo + '/' + filename + '.csv')\n",
    "print(\"Finished compiling CSV\")\n",
    "metadata_dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
